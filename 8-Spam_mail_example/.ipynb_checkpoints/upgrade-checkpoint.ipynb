{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6bYaCABobL5q"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "FlUw7tSKbtg4"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08OTcmxgqkc2"
   },
   "source": [
    "# Automatically upgrade code to TensorFlow 2\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/upgrade\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/upgrade.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/upgrade.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/upgrade.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />\n",
    "    Download notebook</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZSaRPoybOp5"
   },
   "source": [
    "TensorFlow 2.0 includes many API changes, such as reordering arguments, renaming symbols, and changing default values for parameters. Manually performing all of these modifications would be tedious and prone to error. To streamline the changes, and to make your transition to TF 2.0 as seamless as possible, the TensorFlow team has created the `tf_upgrade_v2` utility to help transition legacy code to the new API.\n",
    "\n",
    "Note: `tf_upgrade_v2` is installed automatically for TensorFlow 1.13 and later (including all TF 2.0 builds).\n",
    "\n",
    "Typical usage is like this:\n",
    "\n",
    "<pre class=\"devsite-terminal devsite-click-to-copy prettyprint lang-bsh\">\n",
    "tf_upgrade_v2 \\\n",
    "  --intree my_project/ \\\n",
    "  --outtree my_project_v2/ \\\n",
    "  --reportfile report.txt\n",
    "</pre>\n",
    "\n",
    "It will accelerate your upgrade process by converting existing TensorFlow 1.x Python scripts to TensorFlow 2.0.\n",
    "\n",
    "The conversion script automates as much as possible, but there are still syntactical and stylistic changes that cannot be performed by the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gP9v2vgptdfi"
   },
   "source": [
    "## Compatibility modules\n",
    "\n",
    "Certain API symbols can not be upgraded simply by using a string replacement. To ensure your code is still supported in TensorFlow 2.0, the upgrade script includes a `compat.v1` module. This module replaces TF 1.x symbols like `tf.foo` with the equivalent `tf.compat.v1.foo` reference. While the compatibility module is nice, we recommend that you manually proofread replacements and migrate them to new APIs in the `tf.*` namespace instead of `tf.compat.v1` namespace as quickly as possible.\n",
    "\n",
    "Because of TensorFlow 2.x module deprecations (for example, `tf.flags` and `tf.contrib`), some changes can not be worked around by switching to `compat.v1`. Upgrading this code may require using an additional library (for example, [`absl.flags`](https://github.com/abseil/abseil-py)) or switching to a package in [tensorflow/addons](http://www.github.com/tensorflow/addons).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s78bbfjkXYb7"
   },
   "source": [
    "## Recommended upgrade process\n",
    "\n",
    "The rest of this guide demonstrates how to use the upgrade script. While the upgrade script is easy to use, it is strongly recomended that you use the script as part of the following process: \n",
    "\n",
    "1. **Unit Test**: Ensure that the code you’re upgrading has a unit test suite with reasonable coverage. This is Python code, so the language won’t protect you from many classes of mistakes. Also ensure that any dependency you have has already been upgraded to be compatible with TensorFlow 2.0.\n",
    "\n",
    "1. **Install TensorFlow 1.14**: Upgrade your TensorFlow to the latest TensorFlow 1.x version, at least 1.14. This includes the final TensorFlow 2.0 API in `tf.compat.v2`.\n",
    "\n",
    "1. **Test With 1.14**: Ensure your unit tests pass at this point. You’ll be running them repeatedly as you upgrade so starting from green is important.\n",
    "\n",
    "1. **Run the upgrade script**: Run `tf_upgrade_v2` on your entire source tree, tests included. This will upgrade your code to a format where it only uses symbols available in TensorFlow 2.0. Deprecated symbols will be accessed with `tf.compat.v1`. These will eventually require manual attention, but not immediately.\n",
    "\n",
    "1. **Run the converted tests with TensorFlow 1.14**: Your code should still run fine in TensorFlow 1.14. Run your unit tests again. Any error in your tests here means there’s a bug in the upgrade script. [Please let us know](https://github.com/tensorflow/tensorflow/issues).\n",
    "\n",
    "1. **Check the upgrade report for warnings and errors**: The script writes a report file that explains any conversions you should double-check, or any manual action you need to take. For example: Any remaining instances of contrib will require manual action to remove. Please consult [the RFC for more instructions](https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md). \n",
    "\n",
    "1. **Install TensorFlow 2.0**: At this point it should be safe to switch to TensorFlow 2.0\n",
    "\n",
    "1. **Test with `v1.disable_v2_behavior`**: Re-running your tests with al `v1.disable_v2_behavior()` in the tests main function should give the same results as running under 1.14.\n",
    "\n",
    "1. **Enable V2 Behavior**: Now that your tests work using the v2 API, you can start looking into turning on v2 behavior. Depending on how your code is written this may require some changes. See the [Migration guide](migrate.ipynb) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pwSAQEwvscP"
   },
   "source": [
    "## Using the upgrade script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I9NCvDt5GwX4"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Before getting started ensure that TensorlFlow 2.0 is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWVYbvi1WCeY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ycy3B5PNGutU"
   },
   "source": [
    "Clone the [tensorflow/models](https://github.com/tensorflow/models) git repository so you have some code to test on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jyckoWyAZEhZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'models'...\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 2927, done.\u001b[K\r\n",
      "remote: Counting objects:   0% (1/2927)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Counting objects:   1% (30/2927)\u001b[K\r",
      "remote: Counting objects:   2% (59/2927)\u001b[K\r",
      "remote: Counting objects:   3% (88/2927)\u001b[K\r",
      "remote: Counting objects:   4% (118/2927)\u001b[K\r",
      "remote: Counting objects:   5% (147/2927)\u001b[K\r",
      "remote: Counting objects:   6% (176/2927)\u001b[K\r",
      "remote: Counting objects:   7% (205/2927)\u001b[K\r",
      "remote: Counting objects:   8% (235/2927)\u001b[K\r",
      "remote: Counting objects:   9% (264/2927)\u001b[K\r",
      "remote: Counting objects:  10% (293/2927)\u001b[K\r",
      "remote: Counting objects:  11% (322/2927)\u001b[K\r",
      "remote: Counting objects:  12% (352/2927)\u001b[K\r",
      "remote: Counting objects:  13% (381/2927)\u001b[K\r",
      "remote: Counting objects:  14% (410/2927)\u001b[K\r",
      "remote: Counting objects:  15% (440/2927)\u001b[K\r",
      "remote: Counting objects:  16% (469/2927)\u001b[K\r",
      "remote: Counting objects:  17% (498/2927)\u001b[K\r",
      "remote: Counting objects:  18% (527/2927)\u001b[K\r",
      "remote: Counting objects:  19% (557/2927)\u001b[K\r",
      "remote: Counting objects:  20% (586/2927)\u001b[K\r",
      "remote: Counting objects:  21% (615/2927)\u001b[K\r",
      "remote: Counting objects:  22% (644/2927)\u001b[K\r",
      "remote: Counting objects:  23% (674/2927)\u001b[K\r",
      "remote: Counting objects:  24% (703/2927)\u001b[K\r",
      "remote: Counting objects:  25% (732/2927)\u001b[K\r",
      "remote: Counting objects:  26% (762/2927)\u001b[K\r",
      "remote: Counting objects:  27% (791/2927)\u001b[K\r",
      "remote: Counting objects:  28% (820/2927)\u001b[K\r",
      "remote: Counting objects:  29% (849/2927)\u001b[K\r",
      "remote: Counting objects:  30% (879/2927)\u001b[K\r",
      "remote: Counting objects:  31% (908/2927)\u001b[K\r",
      "remote: Counting objects:  32% (937/2927)\u001b[K\r",
      "remote: Counting objects:  33% (966/2927)\u001b[K\r",
      "remote: Counting objects:  34% (996/2927)\u001b[K\r",
      "remote: Counting objects:  35% (1025/2927)\u001b[K\r",
      "remote: Counting objects:  36% (1054/2927)\u001b[K\r",
      "remote: Counting objects:  37% (1083/2927)\u001b[K\r",
      "remote: Counting objects:  38% (1113/2927)\u001b[K\r",
      "remote: Counting objects:  39% (1142/2927)\u001b[K\r",
      "remote: Counting objects:  40% (1171/2927)\u001b[K\r",
      "remote: Counting objects:  41% (1201/2927)\u001b[K\r",
      "remote: Counting objects:  42% (1230/2927)\u001b[K\r",
      "remote: Counting objects:  43% (1259/2927)\u001b[K\r",
      "remote: Counting objects:  44% (1288/2927)\u001b[K\r",
      "remote: Counting objects:  45% (1318/2927)\u001b[K\r",
      "remote: Counting objects:  46% (1347/2927)\u001b[K\r",
      "remote: Counting objects:  47% (1376/2927)\u001b[K\r",
      "remote: Counting objects:  48% (1405/2927)\u001b[K\r",
      "remote: Counting objects:  49% (1435/2927)\u001b[K\r",
      "remote: Counting objects:  50% (1464/2927)\u001b[K\r",
      "remote: Counting objects:  51% (1493/2927)\u001b[K\r",
      "remote: Counting objects:  52% (1523/2927)\u001b[K\r",
      "remote: Counting objects:  53% (1552/2927)\u001b[K\r",
      "remote: Counting objects:  54% (1581/2927)\u001b[K\r",
      "remote: Counting objects:  55% (1610/2927)\u001b[K\r",
      "remote: Counting objects:  56% (1640/2927)\u001b[K\r",
      "remote: Counting objects:  57% (1669/2927)\u001b[K\r",
      "remote: Counting objects:  58% (1698/2927)\u001b[K\r",
      "remote: Counting objects:  59% (1727/2927)\u001b[K\r",
      "remote: Counting objects:  60% (1757/2927)\u001b[K\r",
      "remote: Counting objects:  61% (1786/2927)\u001b[K\r",
      "remote: Counting objects:  62% (1815/2927)\u001b[K\r",
      "remote: Counting objects:  63% (1845/2927)\u001b[K\r",
      "remote: Counting objects:  64% (1874/2927)\u001b[K\r",
      "remote: Counting objects:  65% (1903/2927)\u001b[K\r",
      "remote: Counting objects:  66% (1932/2927)\u001b[K\r",
      "remote: Counting objects:  67% (1962/2927)\u001b[K\r",
      "remote: Counting objects:  68% (1991/2927)\u001b[K\r",
      "remote: Counting objects:  69% (2020/2927)\u001b[K\r",
      "remote: Counting objects:  70% (2049/2927)\u001b[K\r",
      "remote: Counting objects:  71% (2079/2927)\u001b[K\r",
      "remote: Counting objects:  72% (2108/2927)\u001b[K\r",
      "remote: Counting objects:  73% (2137/2927)\u001b[K\r",
      "remote: Counting objects:  74% (2166/2927)\u001b[K\r",
      "remote: Counting objects:  75% (2196/2927)\u001b[K\r",
      "remote: Counting objects:  76% (2225/2927)\u001b[K\r",
      "remote: Counting objects:  77% (2254/2927)\u001b[K\r",
      "remote: Counting objects:  78% (2284/2927)\u001b[K\r",
      "remote: Counting objects:  79% (2313/2927)\u001b[K\r",
      "remote: Counting objects:  80% (2342/2927)\u001b[K\r",
      "remote: Counting objects:  81% (2371/2927)\u001b[K\r",
      "remote: Counting objects:  82% (2401/2927)\u001b[K\r",
      "remote: Counting objects:  83% (2430/2927)\u001b[K\r",
      "remote: Counting objects:  84% (2459/2927)\u001b[K\r",
      "remote: Counting objects:  85% (2488/2927)\u001b[K\r",
      "remote: Counting objects:  86% (2518/2927)\u001b[K\r",
      "remote: Counting objects:  87% (2547/2927)\u001b[K\r",
      "remote: Counting objects:  88% (2576/2927)\u001b[K\r",
      "remote: Counting objects:  89% (2606/2927)\u001b[K\r",
      "remote: Counting objects:  90% (2635/2927)\u001b[K\r",
      "remote: Counting objects:  91% (2664/2927)\u001b[K\r",
      "remote: Counting objects:  92% (2693/2927)\u001b[K\r",
      "remote: Counting objects:  93% (2723/2927)\u001b[K\r",
      "remote: Counting objects:  94% (2752/2927)\u001b[K\r",
      "remote: Counting objects:  95% (2781/2927)\u001b[K\r",
      "remote: Counting objects:  96% (2810/2927)\u001b[K\r",
      "remote: Counting objects:  97% (2840/2927)\u001b[K\r",
      "remote: Counting objects:  98% (2869/2927)\u001b[K\r",
      "remote: Counting objects:  99% (2898/2927)\u001b[K\r",
      "remote: Counting objects: 100% (2927/2927)\u001b[K\r",
      "remote: Counting objects: 100% (2927/2927), done.\u001b[K\r\n",
      "remote: Compressing objects:   0% (1/2449)\u001b[K\r",
      "remote: Compressing objects:   1% (25/2449)\u001b[K\r",
      "remote: Compressing objects:   2% (49/2449)\u001b[K\r",
      "remote: Compressing objects:   3% (74/2449)\u001b[K\r",
      "remote: Compressing objects:   4% (98/2449)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:   5% (123/2449)\u001b[K\r",
      "remote: Compressing objects:   6% (147/2449)\u001b[K\r",
      "remote: Compressing objects:   7% (172/2449)\u001b[K\r",
      "remote: Compressing objects:   8% (196/2449)\u001b[K\r",
      "remote: Compressing objects:   9% (221/2449)\u001b[K\r",
      "remote: Compressing objects:  10% (245/2449)\u001b[K\r",
      "remote: Compressing objects:  11% (270/2449)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  12% (294/2449)\u001b[K\r",
      "remote: Compressing objects:  13% (319/2449)\u001b[K\r",
      "remote: Compressing objects:  14% (343/2449)\u001b[K\r",
      "remote: Compressing objects:  15% (368/2449)\u001b[K\r",
      "remote: Compressing objects:  16% (392/2449)\u001b[K\r",
      "remote: Compressing objects:  17% (417/2449)\u001b[K\r",
      "remote: Compressing objects:  18% (441/2449)\u001b[K\r",
      "remote: Compressing objects:  19% (466/2449)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  20% (490/2449)\u001b[K\r",
      "remote: Compressing objects:  21% (515/2449)\u001b[K\r",
      "remote: Compressing objects:  22% (539/2449)\u001b[K\r",
      "remote: Compressing objects:  23% (564/2449)\u001b[K\r",
      "remote: Compressing objects:  24% (588/2449)\u001b[K\r",
      "remote: Compressing objects:  25% (613/2449)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  26% (637/2449)\u001b[K\r",
      "remote: Compressing objects:  27% (662/2449)\u001b[K\r",
      "remote: Compressing objects:  28% (686/2449)\u001b[K\r",
      "remote: Compressing objects:  29% (711/2449)\u001b[K\r",
      "remote: Compressing objects:  30% (735/2449)\u001b[K\r",
      "remote: Compressing objects:  31% (760/2449)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  32% (784/2449)\u001b[K\r",
      "remote: Compressing objects:  33% (809/2449)\u001b[K\r",
      "remote: Compressing objects:  34% (833/2449)\u001b[K\r",
      "remote: Compressing objects:  35% (858/2449)\u001b[K\r",
      "remote: Compressing objects:  36% (882/2449)\u001b[K\r",
      "remote: Compressing objects:  37% (907/2449)\u001b[K\r",
      "remote: Compressing objects:  38% (931/2449)\u001b[K\r",
      "remote: Compressing objects:  39% (956/2449)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  40% (980/2449)\u001b[K\r",
      "remote: Compressing objects:  41% (1005/2449)\u001b[K\r",
      "remote: Compressing objects:  42% (1029/2449)\u001b[K\r",
      "remote: Compressing objects:  43% (1054/2449)\u001b[K\r",
      "remote: Compressing objects:  44% (1078/2449)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  45% (1103/2449)\u001b[K\r",
      "remote: Compressing objects:  46% (1127/2449)\u001b[K\r",
      "remote: Compressing objects:  47% (1152/2449)\u001b[K\r",
      "remote: Compressing objects:  48% (1176/2449)\u001b[K\r",
      "remote: Compressing objects:  49% (1201/2449)\u001b[K\r",
      "remote: Compressing objects:  50% (1225/2449)\u001b[K\r",
      "remote: Compressing objects:  51% (1249/2449)\u001b[K\r",
      "remote: Compressing objects:  52% (1274/2449)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  53% (1298/2449)\u001b[K\r",
      "remote: Compressing objects:  54% (1323/2449)\u001b[K\r",
      "remote: Compressing objects:  55% (1347/2449)\u001b[K\r",
      "remote: Compressing objects:  56% (1372/2449)\u001b[K\r",
      "remote: Compressing objects:  57% (1396/2449)\u001b[K\r",
      "remote: Compressing objects:  58% (1421/2449)\u001b[K\r",
      "remote: Compressing objects:  59% (1445/2449)\u001b[K\r",
      "remote: Compressing objects:  60% (1470/2449)\u001b[K\r",
      "remote: Compressing objects:  61% (1494/2449)\u001b[K\r",
      "remote: Compressing objects:  62% (1519/2449)\u001b[K\r",
      "remote: Compressing objects:  63% (1543/2449)\u001b[K\r",
      "remote: Compressing objects:  64% (1568/2449)\u001b[K\r",
      "remote: Compressing objects:  65% (1592/2449)\u001b[K\r",
      "remote: Compressing objects:  66% (1617/2449)\u001b[K\r",
      "remote: Compressing objects:  67% (1641/2449)\u001b[K\r",
      "remote: Compressing objects:  68% (1666/2449)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  69% (1690/2449)\u001b[K\r",
      "remote: Compressing objects:  70% (1715/2449)\u001b[K\r",
      "remote: Compressing objects:  71% (1739/2449)\u001b[K\r",
      "remote: Compressing objects:  72% (1764/2449)\u001b[K\r",
      "remote: Compressing objects:  73% (1788/2449)\u001b[K\r",
      "remote: Compressing objects:  74% (1813/2449)\u001b[K\r",
      "remote: Compressing objects:  75% (1837/2449)\u001b[K\r",
      "remote: Compressing objects:  76% (1862/2449)\u001b[K\r",
      "remote: Compressing objects:  77% (1886/2449)\u001b[K\r",
      "remote: Compressing objects:  78% (1911/2449)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  79% (1935/2449)\u001b[K\r",
      "remote: Compressing objects:  80% (1960/2449)\u001b[K\r",
      "remote: Compressing objects:  81% (1984/2449)\u001b[K\r",
      "remote: Compressing objects:  82% (2009/2449)\u001b[K\r",
      "remote: Compressing objects:  83% (2033/2449)\u001b[K\r",
      "remote: Compressing objects:  84% (2058/2449)\u001b[K\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Compressing objects:  84% (2061/2449)\u001b[K\r",
      "remote: Compressing objects:  85% (2082/2449)\u001b[K\r",
      "remote: Compressing objects:  86% (2107/2449)\u001b[K\r",
      "remote: Compressing objects:  87% (2131/2449)\u001b[K\r",
      "remote: Compressing objects:  88% (2156/2449)\u001b[K\r",
      "remote: Compressing objects:  89% (2180/2449)\u001b[K\r",
      "remote: Compressing objects:  90% (2205/2449)\u001b[K\r",
      "remote: Compressing objects:  91% (2229/2449)\u001b[K\r",
      "remote: Compressing objects:  92% (2254/2449)\u001b[K\r",
      "remote: Compressing objects:  93% (2278/2449)\u001b[K\r",
      "remote: Compressing objects:  94% (2303/2449)\u001b[K\r",
      "remote: Compressing objects:  95% (2327/2449)\u001b[K\r",
      "remote: Compressing objects:  96% (2352/2449)\u001b[K\r",
      "remote: Compressing objects:  97% (2376/2449)\u001b[K\r",
      "remote: Compressing objects:  98% (2401/2449)\u001b[K\r",
      "remote: Compressing objects:  99% (2425/2449)\u001b[K\r",
      "remote: Compressing objects: 100% (2449/2449)\u001b[K\r",
      "remote: Compressing objects: 100% (2449/2449), done.\u001b[K\r\n",
      "Receiving objects:   0% (1/2927)   \r",
      "Receiving objects:   1% (30/2927)   \r",
      "Receiving objects:   2% (59/2927)   \r",
      "Receiving objects:   3% (88/2927)   \r",
      "Receiving objects:   4% (118/2927)   \r",
      "Receiving objects:   5% (147/2927)   \r",
      "Receiving objects:   6% (176/2927)   \r",
      "Receiving objects:   7% (205/2927)   \r",
      "Receiving objects:   8% (235/2927)   \r",
      "Receiving objects:   9% (264/2927)   \r",
      "Receiving objects:  10% (293/2927)   \r",
      "Receiving objects:  11% (322/2927)   \r",
      "Receiving objects:  12% (352/2927)   \r",
      "Receiving objects:  13% (381/2927)   \r",
      "Receiving objects:  14% (410/2927)   \r",
      "Receiving objects:  15% (440/2927)   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  16% (469/2927)   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  17% (498/2927)   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  18% (527/2927)   \r",
      "Receiving objects:  19% (557/2927)   \r",
      "Receiving objects:  20% (586/2927)   \r",
      "Receiving objects:  21% (615/2927)   \r",
      "Receiving objects:  22% (644/2927)   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  22% (662/2927), 7.39 MiB | 6.88 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  23% (674/2927), 7.39 MiB | 6.88 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  24% (703/2927), 7.39 MiB | 6.88 MiB/s   \r",
      "Receiving objects:  25% (732/2927), 7.39 MiB | 6.88 MiB/s   \r",
      "Receiving objects:  26% (762/2927), 7.39 MiB | 6.88 MiB/s   \r",
      "Receiving objects:  27% (791/2927), 7.39 MiB | 6.88 MiB/s   \r",
      "Receiving objects:  28% (820/2927), 7.39 MiB | 6.88 MiB/s   \r",
      "Receiving objects:  29% (849/2927), 7.39 MiB | 6.88 MiB/s   \r",
      "Receiving objects:  30% (879/2927), 7.39 MiB | 6.88 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  31% (908/2927), 13.93 MiB | 8.63 MiB/s   \r",
      "Receiving objects:  32% (937/2927), 13.93 MiB | 8.63 MiB/s   \r",
      "Receiving objects:  33% (966/2927), 13.93 MiB | 8.63 MiB/s   \r",
      "Receiving objects:  34% (996/2927), 13.93 MiB | 8.63 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  35% (1025/2927), 13.93 MiB | 8.63 MiB/s   \r",
      "Receiving objects:  36% (1054/2927), 13.93 MiB | 8.63 MiB/s   \r",
      "Receiving objects:  37% (1083/2927), 13.93 MiB | 8.63 MiB/s   \r",
      "Receiving objects:  38% (1113/2927), 13.93 MiB | 8.63 MiB/s   \r",
      "Receiving objects:  39% (1142/2927), 13.93 MiB | 8.63 MiB/s   \r",
      "Receiving objects:  40% (1171/2927), 13.93 MiB | 8.63 MiB/s   \r",
      "Receiving objects:  41% (1201/2927), 13.93 MiB | 8.63 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  41% (1218/2927), 13.93 MiB | 8.63 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  41% (1225/2927), 34.17 MiB | 10.54 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  41% (1225/2927), 47.66 MiB | 11.02 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  41% (1225/2927), 61.33 MiB | 12.28 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  41% (1229/2927), 75.14 MiB | 12.54 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  42% (1230/2927), 75.14 MiB | 12.54 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  42% (1230/2927), 81.49 MiB | 12.54 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  42% (1230/2927), 94.65 MiB | 12.39 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  42% (1232/2927), 101.07 MiB | 12.29 MiB/s   \r",
      "Receiving objects:  43% (1259/2927), 101.07 MiB | 12.29 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  44% (1288/2927), 107.63 MiB | 12.29 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  45% (1318/2927), 107.63 MiB | 12.29 MiB/s   \r",
      "Receiving objects:  46% (1347/2927), 107.63 MiB | 12.29 MiB/s   \r",
      "Receiving objects:  47% (1376/2927), 107.63 MiB | 12.29 MiB/s   \r",
      "Receiving objects:  48% (1405/2927), 107.63 MiB | 12.29 MiB/s   \r",
      "Receiving objects:  49% (1435/2927), 107.63 MiB | 12.29 MiB/s   \r",
      "Receiving objects:  50% (1464/2927), 107.63 MiB | 12.29 MiB/s   \r",
      "Receiving objects:  51% (1493/2927), 107.63 MiB | 12.29 MiB/s   \r",
      "Receiving objects:  52% (1523/2927), 107.63 MiB | 12.29 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  53% (1552/2927), 107.63 MiB | 12.29 MiB/s   \r",
      "Receiving objects:  54% (1581/2927), 107.63 MiB | 12.29 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  55% (1610/2927), 107.63 MiB | 12.29 MiB/s   \r",
      "Receiving objects:  56% (1640/2927), 107.63 MiB | 12.29 MiB/s   \r",
      "Receiving objects:  57% (1669/2927), 107.63 MiB | 12.29 MiB/s   \r",
      "Receiving objects:  58% (1698/2927), 107.63 MiB | 12.29 MiB/s   \r",
      "Receiving objects:  59% (1727/2927), 107.63 MiB | 12.29 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  59% (1743/2927), 120.78 MiB | 12.18 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  60% (1757/2927), 120.78 MiB | 12.18 MiB/s   \r",
      "Receiving objects:  61% (1786/2927), 120.78 MiB | 12.18 MiB/s   \r",
      "Receiving objects:  62% (1815/2927), 120.78 MiB | 12.18 MiB/s   \r",
      "Receiving objects:  63% (1845/2927), 120.78 MiB | 12.18 MiB/s   \r",
      "Receiving objects:  64% (1874/2927), 120.78 MiB | 12.18 MiB/s   \r",
      "Receiving objects:  65% (1903/2927), 120.78 MiB | 12.18 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  65% (1914/2927), 134.75 MiB | 12.21 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  65% (1914/2927), 148.33 MiB | 12.34 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  65% (1914/2927), 155.10 MiB | 12.39 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  65% (1914/2927), 168.61 MiB | 12.50 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  65% (1914/2927), 182.23 MiB | 12.59 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  65% (1914/2927), 195.89 MiB | 12.53 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  65% (1914/2927), 209.79 MiB | 12.59 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  66% (1932/2927), 216.74 MiB | 12.63 MiB/s   \r",
      "Receiving objects:  67% (1962/2927), 216.74 MiB | 12.63 MiB/s   \r",
      "Receiving objects:  68% (1991/2927), 216.74 MiB | 12.63 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  69% (2020/2927), 216.74 MiB | 12.63 MiB/s   \r",
      "Receiving objects:  70% (2049/2927), 216.74 MiB | 12.63 MiB/s   \r",
      "Receiving objects:  71% (2079/2927), 216.74 MiB | 12.63 MiB/s   \r",
      "Receiving objects:  72% (2108/2927), 216.74 MiB | 12.63 MiB/s   \r",
      "Receiving objects:  73% (2137/2927), 216.74 MiB | 12.63 MiB/s   \r",
      "Receiving objects:  74% (2166/2927), 216.74 MiB | 12.63 MiB/s   \r",
      "Receiving objects:  75% (2196/2927), 216.74 MiB | 12.63 MiB/s   \r",
      "Receiving objects:  76% (2225/2927), 216.74 MiB | 12.63 MiB/s   \r",
      "Receiving objects:  77% (2254/2927), 216.74 MiB | 12.63 MiB/s   \r",
      "Receiving objects:  78% (2284/2927), 216.74 MiB | 12.63 MiB/s   \r",
      "Receiving objects:  79% (2313/2927), 216.74 MiB | 12.63 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  79% (2317/2927), 216.74 MiB | 12.63 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  79% (2327/2927), 230.39 MiB | 12.66 MiB/s   \r",
      "Receiving objects:  80% (2342/2927), 230.39 MiB | 12.66 MiB/s   \r",
      "Receiving objects:  81% (2371/2927), 230.39 MiB | 12.66 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  81% (2374/2927), 243.75 MiB | 12.60 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  81% (2374/2927), 257.07 MiB | 12.53 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  82% (2401/2927), 257.07 MiB | 12.53 MiB/s   \r",
      "Receiving objects:  83% (2430/2927), 257.07 MiB | 12.53 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  83% (2445/2927), 270.75 MiB | 12.49 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  83% (2445/2927), 284.73 MiB | 12.60 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  84% (2459/2927), 284.73 MiB | 12.60 MiB/s   \r",
      "Receiving objects:  85% (2488/2927), 284.73 MiB | 12.60 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  85% (2502/2927), 298.72 MiB | 12.57 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  85% (2511/2927), 305.78 MiB | 12.71 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  86% (2518/2927), 305.78 MiB | 12.71 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  87% (2547/2927), 312.81 MiB | 12.76 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  88% (2576/2927), 312.81 MiB | 12.76 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  89% (2606/2927), 312.81 MiB | 12.76 MiB/s   \r",
      "Receiving objects:  90% (2635/2927), 312.81 MiB | 12.76 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  90% (2640/2927), 319.66 MiB | 12.82 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  90% (2640/2927), 333.71 MiB | 12.90 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  90% (2643/2927), 347.62 MiB | 12.88 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  91% (2664/2927), 354.71 MiB | 12.93 MiB/s   \r",
      "Receiving objects:  92% (2693/2927), 354.71 MiB | 12.93 MiB/s   \r",
      "Receiving objects:  93% (2723/2927), 354.71 MiB | 12.93 MiB/s   \r",
      "Receiving objects:  94% (2752/2927), 354.71 MiB | 12.93 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  94% (2769/2927), 361.80 MiB | 12.91 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  95% (2781/2927), 361.80 MiB | 12.91 MiB/s   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving objects:  96% (2810/2927), 368.81 MiB | 12.90 MiB/s   \r",
      "Receiving objects:  97% (2840/2927), 368.81 MiB | 12.90 MiB/s   \r",
      "Receiving objects:  98% (2869/2927), 368.81 MiB | 12.90 MiB/s   \r",
      "remote: Total 2927 (delta 509), reused 2035 (delta 403), pack-reused 0\u001b[K\r\n",
      "Receiving objects:  99% (2898/2927), 368.81 MiB | 12.90 MiB/s   \r",
      "Receiving objects: 100% (2927/2927), 368.81 MiB | 12.90 MiB/s   \r",
      "Receiving objects: 100% (2927/2927), 369.04 MiB | 12.36 MiB/s, done.\r\n",
      "Resolving deltas:   0% (0/509)   \r",
      "Resolving deltas:   1% (6/509)   \r",
      "Resolving deltas:   2% (11/509)   \r",
      "Resolving deltas:   3% (16/509)   \r",
      "Resolving deltas:   4% (21/509)   \r",
      "Resolving deltas:   5% (29/509)   \r",
      "Resolving deltas:   6% (32/509)   \r",
      "Resolving deltas:   7% (36/509)   \r",
      "Resolving deltas:   8% (41/509)   \r",
      "Resolving deltas:   9% (47/509)   \r",
      "Resolving deltas:  10% (51/509)   \r",
      "Resolving deltas:  11% (56/509)   \r",
      "Resolving deltas:  12% (62/509)   \r",
      "Resolving deltas:  13% (67/509)   \r",
      "Resolving deltas:  14% (73/509)   \r",
      "Resolving deltas:  15% (77/509)   \r",
      "Resolving deltas:  16% (83/509)   \r",
      "Resolving deltas:  17% (87/509)   \r",
      "Resolving deltas:  18% (94/509)   \r",
      "Resolving deltas:  19% (99/509)   \r",
      "Resolving deltas:  20% (103/509)   \r",
      "Resolving deltas:  21% (107/509)   \r",
      "Resolving deltas:  22% (112/509)   \r",
      "Resolving deltas:  23% (118/509)   \r",
      "Resolving deltas:  24% (125/509)   \r",
      "Resolving deltas:  25% (130/509)   \r",
      "Resolving deltas:  26% (135/509)   \r",
      "Resolving deltas:  27% (138/509)   \r",
      "Resolving deltas:  28% (143/509)   \r",
      "Resolving deltas:  29% (149/509)   \r",
      "Resolving deltas:  30% (153/509)   \r",
      "Resolving deltas:  31% (158/509)   \r",
      "Resolving deltas:  32% (163/509)   \r",
      "Resolving deltas:  33% (168/509)   \r",
      "Resolving deltas:  34% (174/509)   \r",
      "Resolving deltas:  35% (183/509)   \r",
      "Resolving deltas:  36% (184/509)   \r",
      "Resolving deltas:  37% (190/509)   \r",
      "Resolving deltas:  38% (194/509)   \r",
      "Resolving deltas:  39% (200/509)   \r",
      "Resolving deltas:  40% (205/509)   \r",
      "Resolving deltas:  41% (209/509)   \r",
      "Resolving deltas:  42% (215/509)   \r",
      "Resolving deltas:  43% (219/509)   \r",
      "Resolving deltas:  44% (225/509)   \r",
      "Resolving deltas:  45% (230/509)   \r",
      "Resolving deltas:  46% (236/509)   \r",
      "Resolving deltas:  47% (240/509)   \r",
      "Resolving deltas:  48% (245/509)   \r",
      "Resolving deltas:  49% (250/509)   \r",
      "Resolving deltas:  50% (256/509)   \r",
      "Resolving deltas:  51% (264/509)   \r",
      "Resolving deltas:  52% (265/509)   \r",
      "Resolving deltas:  53% (270/509)   \r",
      "Resolving deltas:  54% (278/509)   \r",
      "Resolving deltas:  55% (282/509)   \r",
      "Resolving deltas:  56% (286/509)   \r",
      "Resolving deltas:  57% (292/509)   \r",
      "Resolving deltas:  58% (298/509)   \r",
      "Resolving deltas:  59% (304/509)   \r",
      "Resolving deltas:  60% (310/509)   \r",
      "Resolving deltas:  61% (312/509)   \r",
      "Resolving deltas:  62% (319/509)   \r",
      "Resolving deltas:  63% (322/509)   \r",
      "Resolving deltas:  64% (326/509)   \r",
      "Resolving deltas:  65% (335/509)   \r",
      "Resolving deltas:  66% (336/509)   \r",
      "Resolving deltas:  67% (342/509)   \r",
      "Resolving deltas:  68% (348/509)   \r",
      "Resolving deltas:  69% (356/509)   \r",
      "Resolving deltas:  70% (357/509)   \r",
      "Resolving deltas:  71% (362/509)   \r",
      "Resolving deltas:  72% (367/509)   \r",
      "Resolving deltas:  73% (375/509)   \r",
      "Resolving deltas:  74% (379/509)   \r",
      "Resolving deltas:  75% (382/509)   \r",
      "Resolving deltas:  76% (387/509)   \r",
      "Resolving deltas:  77% (392/509)   \r",
      "Resolving deltas:  78% (398/509)   \r",
      "Resolving deltas:  79% (404/509)   \r",
      "Resolving deltas:  80% (408/509)   \r",
      "Resolving deltas:  81% (414/509)   \r",
      "Resolving deltas:  82% (418/509)   \r",
      "Resolving deltas:  83% (423/509)   \r",
      "Resolving deltas:  84% (428/509)   \r",
      "Resolving deltas:  85% (434/509)   \r",
      "Resolving deltas:  86% (440/509)   \r",
      "Resolving deltas:  87% (443/509)   \r",
      "Resolving deltas:  88% (448/509)   \r",
      "Resolving deltas:  89% (455/509)   \r",
      "Resolving deltas:  90% (459/509)   \r",
      "Resolving deltas:  91% (464/509)   \r",
      "Resolving deltas:  92% (470/509)   \r",
      "Resolving deltas:  93% (474/509)   \r",
      "Resolving deltas:  94% (481/509)   \r",
      "Resolving deltas:  95% (486/509)   \r",
      "Resolving deltas:  96% (490/509)   \r",
      "Resolving deltas:  97% (495/509)   \r",
      "Resolving deltas:  98% (500/509)   \r",
      "Resolving deltas:  99% (505/509)   \r",
      "Resolving deltas: 100% (509/509)   \r",
      "Resolving deltas: 100% (509/509), done.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out files:  56% (1576/2768)   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out files:  57% (1578/2768)   \r",
      "Checking out files:  58% (1606/2768)   \r",
      "Checking out files:  59% (1634/2768)   \r",
      "Checking out files:  60% (1661/2768)   \r",
      "Checking out files:  61% (1689/2768)   \r",
      "Checking out files:  62% (1717/2768)   \r",
      "Checking out files:  63% (1744/2768)   \r",
      "Checking out files:  64% (1772/2768)   \r",
      "Checking out files:  65% (1800/2768)   \r",
      "Checking out files:  66% (1827/2768)   \r",
      "Checking out files:  67% (1855/2768)   \r",
      "Checking out files:  68% (1883/2768)   \r",
      "Checking out files:  69% (1910/2768)   \r",
      "Checking out files:  70% (1938/2768)   \r",
      "Checking out files:  71% (1966/2768)   \r",
      "Checking out files:  72% (1993/2768)   \r",
      "Checking out files:  73% (2021/2768)   \r",
      "Checking out files:  74% (2049/2768)   \r",
      "Checking out files:  75% (2076/2768)   \r",
      "Checking out files:  76% (2104/2768)   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out files:  77% (2132/2768)   \r",
      "Checking out files:  78% (2160/2768)   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out files:  79% (2187/2768)   \r",
      "Checking out files:  80% (2215/2768)   \r",
      "Checking out files:  81% (2243/2768)   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out files:  81% (2248/2768)   \r",
      "Checking out files:  82% (2270/2768)   \r",
      "Checking out files:  83% (2298/2768)   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out files:  84% (2326/2768)   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out files:  85% (2353/2768)   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out files:  86% (2381/2768)   \r",
      "Checking out files:  87% (2409/2768)   \r",
      "Checking out files:  88% (2436/2768)   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out files:  89% (2464/2768)   \r",
      "Checking out files:  90% (2492/2768)   \r",
      "Checking out files:  91% (2519/2768)   \r",
      "Checking out files:  92% (2547/2768)   \r",
      "Checking out files:  93% (2575/2768)   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out files:  94% (2602/2768)   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking out files:  95% (2630/2768)   \r",
      "Checking out files:  96% (2658/2768)   \r",
      "Checking out files:  97% (2685/2768)   \r",
      "Checking out files:  98% (2713/2768)   \r",
      "Checking out files:  99% (2741/2768)   \r",
      "Checking out files: 100% (2768/2768)   \r",
      "Checking out files: 100% (2768/2768), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch r1.13.0 --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wfHOhbkgvrKr"
   },
   "source": [
    "### Read the help\n",
    "\n",
    "The script should be installed with TensorFlow. Here is the builtin help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2GF-tlntqTQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: tf_upgrade_v2 [-h] [--infile INPUT_FILE] [--outfile OUTPUT_FILE]\r\n",
      "                     [--intree INPUT_TREE] [--outtree OUTPUT_TREE]\r\n",
      "                     [--copyotherfiles COPY_OTHER_FILES] [--inplace]\r\n",
      "                     [--no_import_rename] [--reportfile REPORT_FILENAME]\r\n",
      "                     [--mode {DEFAULT,SAFETY}] [--print_all]\r\n",
      "\r\n",
      "Convert a TensorFlow Python file from 1.x to 2.0\r\n",
      "\r\n",
      "Simple usage:\r\n",
      "  tf_upgrade_v2.py --infile foo.py --outfile bar.py\r\n",
      "  tf_upgrade_v2.py --infile foo.ipynb --outfile bar.ipynb\r\n",
      "  tf_upgrade_v2.py --intree ~/code/old --outtree ~/code/new\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --infile INPUT_FILE   If converting a single file, the name of the file to\r\n",
      "                        convert\r\n",
      "  --outfile OUTPUT_FILE\r\n",
      "                        If converting a single file, the output filename.\r\n",
      "  --intree INPUT_TREE   If converting a whole tree of files, the directory to\r\n",
      "                        read from (relative or absolute).\r\n",
      "  --outtree OUTPUT_TREE\r\n",
      "                        If converting a whole tree of files, the output\r\n",
      "                        directory (relative or absolute).\r\n",
      "  --copyotherfiles COPY_OTHER_FILES\r\n",
      "                        If converting a whole tree of files, whether to copy\r\n",
      "                        the other files.\r\n",
      "  --inplace             If converting a set of files, whether to allow the\r\n",
      "                        conversion to be performed on the input files.\r\n",
      "  --no_import_rename    Not to rename import to compact.v2 explicitly.\r\n",
      "  --reportfile REPORT_FILENAME\r\n",
      "                        The name of the file where the report log is\r\n",
      "                        stored.(default: report.txt)\r\n",
      "  --mode {DEFAULT,SAFETY}\r\n",
      "                        Upgrade script mode. Supported modes: DEFAULT: Perform\r\n",
      "                        only straightforward conversions to upgrade to 2.0. In\r\n",
      "                        more difficult cases, switch to use compat.v1. SAFETY:\r\n",
      "                        Keep 1.* code intact and import compat.v1 module.\r\n",
      "  --print_all           Print full log to stdout instead of just printing\r\n",
      "                        errors\r\n"
     ]
    }
   ],
   "source": [
    "!tf_upgrade_v2 -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "se9Leqjm1CZR"
   },
   "source": [
    "### Example TF1 code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "whD5i36s1SuM"
   },
   "source": [
    "Here is a simple TensorFlow 1.0 script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mhGbYQ9HwbeU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  # Calculate loss using mean squared error\r\n",
      "  average_loss = tf.losses.mean_squared_error(labels, predictions)\r\n",
      "\r\n",
      "  # Pre-made estimators use the total_loss instead of the average,\r\n",
      "  # so report total_loss for compatibility.\r\n",
      "  batch_size = tf.shape(labels)[0]\r\n",
      "  total_loss = tf.to_float(batch_size) * average_loss\r\n",
      "\r\n",
      "  if mode == tf.estimator.ModeKeys.TRAIN:\r\n",
      "    optimizer = params.get(\"optimizer\", tf.train.AdamOptimizer)\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 65 models/samples/cookbook/regression/custom_regression.py | tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGO7xSyL89wX"
   },
   "source": [
    "With TensorFlow 2.0 installed it does not run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TD7fFphX8_qE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"custom_regression.py\", line 162, in <module>\r\n",
      "    tf.logging.set_verbosity(tf.logging.INFO)\r\n",
      "AttributeError: module 'tensorflow' has no attribute 'logging'\r\n"
     ]
    }
   ],
   "source": [
    "!(cd models/samples/cookbook/regression && python custom_regression.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZZHu0H0wLRJ"
   },
   "source": [
    "### Single file\n",
    "\n",
    "The upgrade script can be run on a single Python file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [
      "sh"
     ],
     "id": ""
    },
    "colab": {},
    "colab_type": "code",
    "id": "xIBZVEjkqkc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO line 38:8: Renamed 'tf.feature_column.input_layer' to 'tf.compat.v1.feature_column.input_layer'\r\n",
      "INFO line 43:10: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\r\n",
      "INFO line 46:17: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\r\n",
      "INFO line 57:17: tf.losses.mean_squared_error requires manual check. tf.losses have been replaced with object oriented versions in TF 2.0 and after. The loss function calls have been converted to compat.v1 for backward compatibility. Please update these calls to the TF 2.0 versions.\r\n",
      "INFO line 57:17: Renamed 'tf.losses.mean_squared_error' to 'tf.compat.v1.losses.mean_squared_error'\r\n",
      "INFO line 61:15: Added keywords to args of function 'tf.shape'\r\n",
      "INFO line 62:15: Changed tf.to_float call to tf.cast(..., dtype=tf.float32).\r\n",
      "INFO line 65:40: Renamed 'tf.train.AdamOptimizer' to 'tf.compat.v1.train.AdamOptimizer'\r\n",
      "INFO line 68:39: Renamed 'tf.train.get_global_step' to 'tf.compat.v1.train.get_global_step'\r\n",
      "INFO line 83:9: tf.metrics.root_mean_squared_error requires manual check. tf.metrics have been replaced with object oriented versions in TF 2.0 and after. The metric function calls have been converted to compat.v1 for backward compatibility. Please update these calls to the TF 2.0 versions.\r\n",
      "INFO line 83:9: Renamed 'tf.metrics.root_mean_squared_error' to 'tf.compat.v1.metrics.root_mean_squared_error'\r\n",
      "INFO line 142:23: Renamed 'tf.train.AdamOptimizer' to 'tf.compat.v1.train.AdamOptimizer'\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO line 162:2: Renamed 'tf.logging.set_verbosity' to 'tf.compat.v1.logging.set_verbosity'\r\n",
      "INFO line 162:27: Renamed 'tf.logging.INFO' to 'tf.compat.v1.logging.INFO'\r\n",
      "INFO line 163:2: Renamed 'tf.app.run' to 'tf.compat.v1.app.run'\r\n",
      "TensorFlow 2.0 Upgrade Script\r\n",
      "-----------------------------\r\n",
      "Converted 1 files\r\n",
      "Detected 0 issues that require attention\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "\r\n",
      "\r\n",
      "Make sure to read the detailed log 'report.txt'\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!tf_upgrade_v2 \\\n",
    "  --infile models/samples/cookbook/regression/custom_regression.py \\\n",
    "  --outfile /tmp/custom_regression_v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9X2lxzqqkc9"
   },
   "source": [
    "The script will print errors if it can not find a fix for the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r7zpuE1vWSlL"
   },
   "source": [
    "### Directory tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2q7Gtuu8SdIC"
   },
   "source": [
    "Typical projects, including this simple example, will use much more than one file. Typically want to upgrade an entire package, so the script can also be run on a directory tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGqcdkAPqkc-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO line 72:10: tf.estimator.DNNRegressor: Default value of loss_reduction has been changed to SUM_OVER_BATCH_SIZE; inserting old default value tf.keras.losses.Reduction.SUM.\r\n",
      "\r\n",
      "INFO line 96:2: Renamed 'tf.logging.set_verbosity' to 'tf.compat.v1.logging.set_verbosity'\r\n",
      "INFO line 96:27: Renamed 'tf.logging.INFO' to 'tf.compat.v1.logging.INFO'\r\n",
      "INFO line 97:2: Renamed 'tf.app.run' to 'tf.compat.v1.app.run'\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO line 38:8: Renamed 'tf.feature_column.input_layer' to 'tf.compat.v1.feature_column.input_layer'\r\n",
      "INFO line 43:10: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\r\n",
      "INFO line 46:17: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\r\n",
      "INFO line 57:17: tf.losses.mean_squared_error requires manual check. tf.losses have been replaced with object oriented versions in TF 2.0 and after. The loss function calls have been converted to compat.v1 for backward compatibility. Please update these calls to the TF 2.0 versions.\r\n",
      "INFO line 57:17: Renamed 'tf.losses.mean_squared_error' to 'tf.compat.v1.losses.mean_squared_error'\r\n",
      "INFO line 61:15: Added keywords to args of function 'tf.shape'\r\n",
      "INFO line 62:15: Changed tf.to_float call to tf.cast(..., dtype=tf.float32).\r\n",
      "INFO line 65:40: Renamed 'tf.train.AdamOptimizer' to 'tf.compat.v1.train.AdamOptimizer'\r\n",
      "INFO line 68:39: Renamed 'tf.train.get_global_step' to 'tf.compat.v1.train.get_global_step'\r\n",
      "INFO line 83:9: tf.metrics.root_mean_squared_error requires manual check. tf.metrics have been replaced with object oriented versions in TF 2.0 and after. The metric function calls have been converted to compat.v1 for backward compatibility. Please update these calls to the TF 2.0 versions.\r\n",
      "INFO line 83:9: Renamed 'tf.metrics.root_mean_squared_error' to 'tf.compat.v1.metrics.root_mean_squared_error'\r\n",
      "INFO line 142:23: Renamed 'tf.train.AdamOptimizer' to 'tf.compat.v1.train.AdamOptimizer'\r\n",
      "INFO line 162:2: Renamed 'tf.logging.set_verbosity' to 'tf.compat.v1.logging.set_verbosity'\r\n",
      "INFO line 162:27: Renamed 'tf.logging.INFO' to 'tf.compat.v1.logging.INFO'\r\n",
      "INFO line 163:2: Renamed 'tf.app.run' to 'tf.compat.v1.app.run'\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO line 82:10: tf.estimator.LinearRegressor: Default value of loss_reduction has been changed to SUM_OVER_BATCH_SIZE; inserting old default value tf.keras.losses.Reduction.SUM.\r\n",
      "\r\n",
      "INFO line 105:2: Renamed 'tf.logging.set_verbosity' to 'tf.compat.v1.logging.set_verbosity'\r\n",
      "INFO line 105:27: Renamed 'tf.logging.INFO' to 'tf.compat.v1.logging.INFO'\r\n",
      "INFO line 106:2: Renamed 'tf.app.run' to 'tf.compat.v1.app.run'\r\n",
      "INFO line 58:10: tf.estimator.LinearRegressor: Default value of loss_reduction has been changed to SUM_OVER_BATCH_SIZE; inserting old default value tf.keras.losses.Reduction.SUM.\r\n",
      "\r\n",
      "INFO line 101:2: Renamed 'tf.logging.set_verbosity' to 'tf.compat.v1.logging.set_verbosity'\r\n",
      "INFO line 101:27: Renamed 'tf.logging.INFO' to 'tf.compat.v1.logging.INFO'\r\n",
      "INFO line 102:2: Renamed 'tf.app.run' to 'tf.compat.v1.app.run'\r\n",
      "WARNING line 125:15: Changing dataset.make_one_shot_iterator() to tf.compat.v1.data.make_one_shot_iterator(dataset). Please check this transformation.\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO line 40:7: Renamed 'tf.test.mock' to 'tf.compat.v1.test.mock'\r\n",
      "TensorFlow 2.0 Upgrade Script\r\n",
      "-----------------------------\r\n",
      "Converted 7 files\r\n",
      "Detected 1 issues that require attention\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "File: models/samples/cookbook/regression/automobile_data.py\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "models/samples/cookbook/regression/automobile_data.py:125:15: WARNING: Changing dataset.make_one_shot_iterator() to tf.compat.v1.data.make_one_shot_iterator(dataset). Please check this transformation.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Make sure to read the detailed log 'tree_report.txt'\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# upgrade the .py files and copy all the other files to the outtree\n",
    "!tf_upgrade_v2 \\\n",
    "    --intree models/samples/cookbook/regression/ \\\n",
    "    --outtree regression_v2/ \\\n",
    "    --reportfile tree_report.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2S4j7sqbSowC"
   },
   "source": [
    "Note the one warning about the `dataset.make_one_shot_iterator` function.\n",
    "\n",
    "Now the script works in with TensorFlow 2.0:\n",
    "\n",
    "Note that because the `tf.compat.v1` module, the converted script will also run in TensorFlow 1.14. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vh0cmW3y1tX9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0616 22:50:55.960274 139842388465472 estimator.py:2066] Saving dict for global step 1000: global_step = 1000, loss = 465.60657, rmse = 3.1145046\r\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /tmp/tmpad5oxh79/model.ckpt-1000\r\n",
      "I0616 22:50:55.999639 139842388465472 estimator.py:2127] Saving 'checkpoint_path' summary for global step 1000: /tmp/tmpad5oxh79/model.ckpt-1000\r\n",
      "Tensor(\"IteratorGetNext:25\", shape=(None,), dtype=float64, device=/device:CPU:0)\r\n",
      "Tensor(\"Squeeze:0\", shape=(None,), dtype=float32)\r\n",
      "\r\n",
      "********************************************************************************\r\n",
      "\r\n",
      "RMS error for the test set: $3115\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!(cd regression_v2 && python custom_regression.py 2>&1) | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4EgZGGkdqkdC"
   },
   "source": [
    "## Detailed report\n",
    "\n",
    "The script also reports a list of detailed changes. In this example it found one possibly unsafe transformation and included a warning at the top of the file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CtHaZbVaNMGV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.0 Upgrade Script\r\n",
      "-----------------------------\r\n",
      "Converted 7 files\r\n",
      "Detected 1 issues that require attention\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "File: models/samples/cookbook/regression/automobile_data.py\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "models/samples/cookbook/regression/automobile_data.py:125:15: WARNING: Changing dataset.make_one_shot_iterator() to tf.compat.v1.data.make_one_shot_iterator(dataset). Please check this transformation.\r\n",
      "\r\n",
      "================================================================================\r\n",
      "Detailed log follows:\r\n",
      "\r\n",
      "================================================================================\r\n",
      "================================================================================\r\n",
      "Input tree: 'models/samples/cookbook/regression/'\r\n",
      "================================================================================\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Processing file 'models/samples/cookbook/regression/dnn_regression.py'\r\n",
      " outputting to 'regression_v2/dnn_regression.py'\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 20 tree_report.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-UIFXP3cFSa"
   },
   "source": [
    "Note again the one warning about the `Dataset.make_one_shot_iterator function`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxQeYS1TN-jv"
   },
   "source": [
    "In other cases the output will explain the reasoning for non-trivial changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQs9kEvVN9th"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dropout.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dropout.py\n",
    "import tensorflow as tf\n",
    "\n",
    "d = tf.nn.dropout(tf.range(10), 0.2)\n",
    "z = tf.zeros_like(d, optimize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7uOkacZsO3XX"
   },
   "outputs": [],
   "source": [
    "!tf_upgrade_v2 \\\n",
    "  --infile dropout.py \\\n",
    "  --outfile dropout_v2.py \\\n",
    "  --reportfile dropout_report.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-J82-scPMGl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.0 Upgrade Script\r\n",
      "-----------------------------\r\n",
      "Converted 1 files\r\n",
      "Detected 0 issues that require attention\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "================================================================================\r\n",
      "Detailed log follows:\r\n",
      "\r\n",
      "================================================================================\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Processing file 'dropout.py'\r\n",
      " outputting to 'dropout_v2.py'\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "\r\n",
      "3:4: INFO: Changing keep_prob arg of tf.nn.dropout to rate, and recomputing value.\r\n",
      "\r\n",
      "4:4: INFO: Renaming tf.zeros_like to tf.compat.v1.zeros_like because argument optimize is present. tf.zeros_like no longer takes an optimize argument, and behaves as if optimize=True. This call site specifies something other than optimize=True, so it was converted to compat.v1.\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat dropout_report.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOOLN21nTGSS"
   },
   "source": [
    "Here is the modified file contents, note how the script adds argument names to deal with moved and renamed arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SrYcJk9-TFlU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import tensorflow as tf\r\n",
      "\r\n",
      "d = tf.nn.dropout(tf.range(10), 1 - (0.2))\r\n",
      "z = tf.compat.v1.zeros_like(d, optimize=False)\r\n"
     ]
    }
   ],
   "source": [
    "!cat dropout_v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wI_sVNp_b4C4"
   },
   "source": [
    "A larger project might contain a few errors. For example convert the deeplab model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uzuY-bOvYBS7"
   },
   "outputs": [],
   "source": [
    "!tf_upgrade_v2 \\\n",
    "    --intree models/research/deeplab \\\n",
    "    --outtree deeplab_v2 \\\n",
    "    --reportfile deeplab_report.txt > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLhw3fm8drae"
   },
   "source": [
    "It produced the output files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YYLRxWJdSvQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\tdatasets\t    input_preprocess.py        train.py\r\n",
      "__init__.py\tdeeplab_demo.ipynb  local_test.sh\t       utils\r\n",
      "common.py\teval.py\t\t    local_test_mobilenetv2.sh  vis.py\r\n",
      "common_test.py\texport_model.py     model.py\r\n",
      "core\t\tg3doc\t\t    model_test.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls deeplab_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtTC-cAZdEBy"
   },
   "source": [
    "But there were errors. The report will help you pin-point what you need to fix before this will run. Here are the first three errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UVTNOohlcyVZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/research/deeplab/train.py:29:7: ERROR: Using member tf.contrib.slim in deprecated module tf.contrib. tf.contrib.slim cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\n",
      "models/research/deeplab/vis.py:31:7: ERROR: Using member tf.contrib.slim in deprecated module tf.contrib. tf.contrib.slim cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\n",
      "models/research/deeplab/export_model.py:25:7: ERROR: Using member tf.contrib.slim in deprecated module tf.contrib. tf.contrib.slim cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\n"
     ]
    }
   ],
   "source": [
    "!cat deeplab_report.txt | grep -i models/research/deeplab | grep -i error | head -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGBeDaFVRJ5l"
   },
   "source": [
    "## \"Safety\" mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BnfCxB7SVtTO"
   },
   "source": [
    "The conversion script also has a less invasive `SAFETY` mode that simply changes the imports to use the `tensorflow.compat.v1` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XdaVXCPWQCC5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import tensorflow as tf\r\n",
      "\r\n",
      "d = tf.nn.dropout(tf.range(10), 0.2)\r\n",
      "z = tf.zeros_like(d, optimize=False)\r\n"
     ]
    }
   ],
   "source": [
    "!cat dropout.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0tvRJLGRYEb"
   },
   "outputs": [],
   "source": [
    "!tf_upgrade_v2 --mode SAFETY --infile dropout.py --outfile dropout_v2_safe.py > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "91suN2RaRfIV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import tensorflow.compat.v1 as tf\r\n",
      "\r\n",
      "d = tf.nn.dropout(tf.range(10), 0.2)\r\n",
      "z = tf.zeros_like(d, optimize=False)\r\n"
     ]
    }
   ],
   "source": [
    "!cat dropout_v2_safe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOzTF7xbZqqW"
   },
   "source": [
    "As you can see this doesn't upgrade your code, but does allow TensorFlow 1 code to run in TensorFlow 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jGfXVApkqkdG"
   },
   "source": [
    "## Caveats\n",
    "\n",
    "- Do not update parts of your code manually before running this script. In particular, functions that have had reordered arguments like `tf.argmax` or `tf.batch_to_space` cause the script to incorrectly add keyword arguments that mismap your existing code.\n",
    "\n",
    "- The script assumes that `tensorflow` is imported using `import tensorflow as tf`.\n",
    "\n",
    "- This script does not reorder arguments. Instead, the script adds keyword arguments to functions that have their arguments reordered.\n",
    "\n",
    "- Check out [tf2up.ml](http://tf2up.ml) for a convenient tool to upgrade Jupyter\n",
    "  notebooks and Python files in a GitHub repository.\n",
    "\n",
    "To report upgrade script bugs or make feature requests, please file an issue on [GitHub](https://github.com/tensorflow/tensorflow/issues). And if you’re testing TensorFlow 2.0, we want to hear about it! Join the [TF 2.0 Testing community](https://groups.google.com/a/tensorflow.org/forum/#!forum/testing) and send questions and discussion to [testing@tensorflow.org](mailto:testing@tensorflow.org)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "upgrade.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
